{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VSreLGZEbcC"
      },
      "source": [
        "First, mount your google drive here to access the dataset folder later."
      ],
      "id": "1VSreLGZEbcC"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "LoWcmobVFCyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe88317-804c-4a6d-93bc-077ec45a485c"
      },
      "id": "LoWcmobVFCyN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all the necessary packages."
      ],
      "metadata": {
        "id": "9N7DqRIEfYpY"
      },
      "id": "9N7DqRIEfYpY"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ksUT7kjCEbcD"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "from moviepy.editor import *\n",
        "import random\n",
        "from PIL import Image\n",
        "import PIL.ImageOps"
      ],
      "id": "ksUT7kjCEbcD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUjXShg4EbcF"
      },
      "source": [
        "Now load some pytorch functions."
      ],
      "id": "SUjXShg4EbcF"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F87iMv2lEbcF"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.utils\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch.nn import init"
      ],
      "id": "F87iMv2lEbcF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if gpu is available. If the result is False, change runtime type in Runtime-->Change runtime type-->Hardware accelerator \"T4 GPU\""
      ],
      "metadata": {
        "id": "Urwsd-Z_Vsms"
      },
      "id": "Urwsd-Z_Vsms"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFDKixNQKL-4",
        "outputId": "7acf0bc6-335f-4a0d-fc87-cbe972d8bb2c"
      },
      "id": "PFDKixNQKL-4",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6BRyB-WEbcG"
      },
      "source": [
        "This is the xception model that we will use. It takes inputted images in size 299x299, and outputs a vector of size 1000, which will be the feature map that encodes information about the inputted diagrams.\n",
        "\n",
        "One important thing is to change the path to the pretrained model based on your mount location. Find the line:\n",
        "\n",
        "\n",
        "```\n",
        "if pretrained:\n",
        "        model.load_state_dict(torch.load('/content/drive/MyDrive/Music_Plagiarism/xception-43020ad28.pth'))\n",
        "```\n",
        "Change the path inside ```torch.load()``` to the path pointing to ```xception-43020ad28.pth``` in our shared folder. This is the pretrained parameters trained on imagenet data.\n"
      ],
      "id": "v6BRyB-WEbcG"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-BVh8TYfEbcG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates an Xception Model as defined in:\n",
        "\n",
        "Francois Chollet\n",
        "Xception: Deep Learning with Depthwise Separable Convolutions\n",
        "https://arxiv.org/pdf/1610.02357.pdf\n",
        "\n",
        "This weights ported from the Keras implementation. Achieves the following performance on the validation set:\n",
        "\n",
        "Loss:0.9173 Prec@1:78.892 Prec@5:94.292\n",
        "\n",
        "REMEMBER to set your image size to 3x299x299 for both test and validation\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                  std=[0.5, 0.5, 0.5])\n",
        "\n",
        "The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n",
        "\"\"\"\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch.nn import init\n",
        "import torch\n",
        "\n",
        "__all__ = ['xception']\n",
        "\n",
        "model_urls = {\n",
        "#     'xception':'https://www.dropbox.com/s/1hplpzet9d7dv29/xception-c0a72b38.pth.tar?dl=1'\n",
        "    'xception':'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth'\n",
        "}\n",
        "\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        if out_filters != in_filters or strides!=1:\n",
        "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
        "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
        "        else:\n",
        "            self.skip=None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        rep=[]\n",
        "\n",
        "        filters=in_filters\n",
        "        if grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "            filters = out_filters\n",
        "\n",
        "        for i in range(reps-1):\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(filters))\n",
        "\n",
        "        if not grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "\n",
        "        if not start_with_relu:\n",
        "            rep = rep[1:]\n",
        "        else:\n",
        "            rep[0] = nn.ReLU(inplace=False)\n",
        "\n",
        "        if strides != 1:\n",
        "            rep.append(nn.MaxPool2d(3,strides,1))\n",
        "        self.rep = nn.Sequential(*rep)\n",
        "\n",
        "    def forward(self,inp):\n",
        "        x = self.rep(inp)\n",
        "\n",
        "        if self.skip is not None:\n",
        "            skip = self.skip(inp)\n",
        "            skip = self.skipbn(skip)\n",
        "        else:\n",
        "            skip = inp\n",
        "\n",
        "        x+=skip\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Xception(nn.Module):\n",
        "    \"\"\"\n",
        "    Xception optimized for the ImageNet dataset, as specified in\n",
        "    https://arxiv.org/pdf/1610.02357.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=1000):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            num_classes: number of classes\n",
        "        \"\"\"\n",
        "        super(Xception, self).__init__()\n",
        "\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        #do relu here\n",
        "\n",
        "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
        "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
        "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
        "\n",
        "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
        "        self.bn3 = nn.BatchNorm2d(1536)\n",
        "\n",
        "        #do relu here\n",
        "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
        "        self.bn4 = nn.BatchNorm2d(2048)\n",
        "\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "        #------- init weights --------\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        #-----------------------------\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "        x = self.block8(x)\n",
        "        x = self.block9(x)\n",
        "        x = self.block10(x)\n",
        "        x = self.block11(x)\n",
        "        x = self.block12(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "        # In this function we pass in both images and obtain both vectors\n",
        "        # which are returned\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        output3 = self.forward_once(input3)\n",
        "\n",
        "        return output1, output2, output3\n",
        "\n",
        "# Define the triplet Loss Function\n",
        "class TripletLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    # output 1 for anchor, output2 for positive, output3 for negative\n",
        "    def forward(self, output1, output2, output3):\n",
        "      # Calculate the euclidean distance and calculate the contrastive loss\n",
        "      euclidean_distance_p = F.pairwise_distance(output1, output2, keepdim = True)\n",
        "      euclidean_distance_n = F.pairwise_distance(output1, output3, keepdim = True)\n",
        "\n",
        "      loss_triple = torch.mean((torch.clamp(self.margin - euclidean_distance_n + euclidean_distance_p, min=0.0)))\n",
        "\n",
        "      return loss_triple\n",
        "\n",
        "def xception(pretrained=True, to_cuda = False, reload_previous = False, **kwargs):\n",
        "    \"\"\"\n",
        "    Construct Xception.\n",
        "    \"\"\"\n",
        "\n",
        "    if to_cuda:\n",
        "        model = Xception().cuda()\n",
        "    else:\n",
        "        model = Xception()\n",
        "    if pretrained:\n",
        "      # CHANGE HERE(path to )\n",
        "        model.load_state_dict(torch.load('/content/drive/MyDrive/Music_Plagiarism/xception-43020ad28.pth'))\n",
        "        # model.load_state_dict(model_zoo.load_url(model_urls['xception']))\n",
        "    if reload_previous:\n",
        "        load_model(model, path = kwargs['RELOAD_PATH'])\n",
        "    return model\n",
        "\n",
        "def load_model(model, path = './xception.pth'):\n",
        "    model.load_state_dict(torch.load(path))"
      ],
      "id": "-BVh8TYfEbcG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgPqoNuCEbcH"
      },
      "source": [
        "Dataset loader for training"
      ],
      "id": "HgPqoNuCEbcH"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6JJgDRfjEbcI"
      },
      "outputs": [],
      "source": [
        "class NetworkDataset(Dataset):\n",
        "    def __init__(self,imageFolderDataset,transform=None):\n",
        "        self.imageFolderDataset = imageFolderDataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "\n",
        "        #We need to get a image in the same class and in a different class\n",
        "        while True:\n",
        "            #Look untill the same class image is found\n",
        "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "            if img0_tuple[1] == img1_tuple[1]:\n",
        "                break\n",
        "        while True:\n",
        "            #Look untill a different class image is found\n",
        "            img2_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "            if img0_tuple[1] != img2_tuple[1]:\n",
        "                break\n",
        "\n",
        "        # open image file\n",
        "        img0 = Image.open(img0_tuple[0])\n",
        "        img1 = Image.open(img1_tuple[0])\n",
        "        img2 = Image.open(img2_tuple[0])\n",
        "\n",
        "        # to RBG\n",
        "        img0 = img0.convert(\"RGB\")\n",
        "        img1 = img1.convert(\"RGB\")\n",
        "        img2 = img2.convert(\"RGB\")\n",
        "\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img0, img1, img2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imageFolderDataset.imgs)"
      ],
      "id": "6JJgDRfjEbcI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some helper functions needed for audio processing and etc..\n",
        "\n"
      ],
      "metadata": {
        "id": "PpXBGOrOeilt"
      },
      "id": "PpXBGOrOeilt"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MGr7ViGnEbcJ"
      },
      "outputs": [],
      "source": [
        "# ------------ string related\n",
        "def convert_time_to_sec(time_str):\n",
        "    # convert time string like ['1:27'] to seconds\n",
        "    if \",\" in time_str:\n",
        "        return [convert_time_to_sec(i) for i in time_str.split(\",\")]\n",
        "\n",
        "    # if \"[\" in time_str:\n",
        "    time_str = time_str.replace(\"[\", \"\")\n",
        "    time_str = time_str.replace(\"]\", \"\")\n",
        "    time_str = time_str.replace(\"'\", \"\")\n",
        "\n",
        "    if \":\" in time_str:\n",
        "        time_str = time_str.split(\":\")\n",
        "    elif \"_\" in time_str:\n",
        "        time_str = time_str.split(\"_\")\n",
        "    return int(time_str[0]) * 60 + int(time_str[1])\n",
        "\n",
        "def replace_invalid_char(string):\n",
        "\tinvalid_char = ['/',':','*','?','\"','<','>','|']\n",
        "\tnew_string = ''\n",
        "\tfor s in string:\n",
        "\t\tif s in invalid_char:\n",
        "\t\t\ts = '_'\n",
        "\t\tnew_string += s\n",
        "\treturn new_string\n",
        "\n",
        "def remove_special_char(string):\n",
        "\ttest_str = ''.join(letter for letter in string if letter.isalnum())\n",
        "\treturn test_str\n",
        "#--------------------------------------------\n",
        "\n",
        "\n",
        "# ------------------------- audio related\n",
        "def stretch_audio(audio, sr, rate):\n",
        "\tnew_audio = librosa.effects.time_stretch(audio, rate=rate)\n",
        "\treturn new_audio\n",
        "\n",
        "def shift_audio(audio, sr, semitone):\n",
        "\ty_shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=semitone)\n",
        "\treturn y_shifted\n",
        "\n",
        "def bpm_estimation(audio, sr):\n",
        "\tbpm, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
        "\treturn bpm\n",
        "\n",
        "def calculate_eightbars_duration(audio, sr):\n",
        "\tbpm = bpm_estimation(audio, sr)\n",
        "\t# assuming a 44 time signature\n",
        "\tsecs = 60/bpm*4*8\n",
        "\treturn secs\n",
        "# --------------------------------------\n",
        "\n",
        "# ---------------------for dataset conversion\n",
        "def list_of_existing_sampletime(dir_list):\n",
        "    return np.array(dir_list, dtype = np.int32)\n",
        "\n",
        "def find_closest_number(array, target):\n",
        "  # Find the minimum distance between the target value and each number in the array.\n",
        "  distances = [np.abs(target - number) for number in array]\n",
        "  # Return the index of the number with the minimum distance.\n",
        "  return distances.index(min(distances))\n",
        "\n",
        "def save_mel_spectrogram(audio_clip, file_path):\n",
        "    # Produce the mel-spectrogram\n",
        "    S = librosa.feature.melspectrogram(y=audio_clip, sr=SAMPLE_RATE)\n",
        "    S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Save the mel-spectrogram\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    librosa.display.specshow(S_DB, sr=SAMPLE_RATE)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path, bbox_inches='tight', pad_inches = 0, transparent = True)\n",
        "    plt.close()\n",
        "\n",
        "def save_chroma_feature(audio_clip, file_path):\n",
        "     # Produce the chroma feature\n",
        "    S = np.abs(librosa.stft(audio_clip, n_fft=4096))**2\n",
        "    S = librosa.feature.chroma_stft(y=audio_clip, sr=SAMPLE_RATE)\n",
        "    chroma = librosa.amplitude_to_db(S, ref=np.max)\n",
        "\n",
        "    # Save the chroma feature\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    librosa.display.specshow(chroma, sr=SAMPLE_RATE)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path, bbox_inches='tight', pad_inches = 0, transparent = True)\n",
        "    plt.close()\n",
        "\n",
        "def get_mel_spectrogram(audio_clip):\n",
        "    S = librosa.feature.melspectrogram(y=audio_clip, sr=SAMPLE_RATE)\n",
        "    S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    librosa.display.specshow(S_DB, sr=SAMPLE_RATE)\n",
        "    plt.tight_layout()\n",
        "    tempname = \"/content/drive/MyDrive/Music_Plagiarism/temp_{0}.png\".format(time.time())\n",
        "    plt.savefig(tempname, bbox_inches='tight', pad_inches = 0, transparent = True)\n",
        "    img = Image.open(tempname)\n",
        "    img = img.convert(\"RGB\")\n",
        "    os.remove(tempname)\n",
        "    plt.close()\n",
        "    return img\n",
        "\n",
        "def get_chroma_feature(audio_clip):\n",
        "    S = np.abs(librosa.stft(audio_clip, n_fft=4096))**2\n",
        "    S = librosa.feature.chroma_stft(y=audio_clip, sr=SAMPLE_RATE)\n",
        "    chroma = librosa.amplitude_to_db(S, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    librosa.display.specshow(chroma, sr=SAMPLE_RATE)\n",
        "    plt.tight_layout()\n",
        "    tempname = \"./temp_{0}.png\".format(time.time())\n",
        "    plt.savefig(tempname, bbox_inches='tight', pad_inches = 0, transparent = True)\n",
        "    img = Image.open(tempname)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img.show()\n",
        "    os.remove(tempname)\n",
        "    plt.close()\n",
        "    return img\n",
        "#--------------------------------------------\n",
        "\n",
        "# ------------------------ for images\n",
        "# Showing images\n",
        "def imshow(img, text=None):\n",
        "\tnpimg = img.numpy()\n",
        "\tplt.axis(\"off\")\n",
        "\tif text:\n",
        "\t\tplt.text(75, 8, text, style='italic',fontweight='bold',\n",
        "\t\t\tbbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
        "\n",
        "\tplt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\tplt.show()\n",
        "\n",
        "# Plotting data\n",
        "def show_plot(iteration,loss):\n",
        "\tplt.plot(iteration,loss)\n",
        "\tplt.show()\n",
        "\n",
        "#-------------------------------------------\n",
        "\n",
        "# -------------- similarity score functions\n",
        "def cos_sim_score(output1, output2):\n",
        "\tscore = F.cosine_similarity(output1,output2, dim = 1)\n",
        "\treturn score\n",
        "\n",
        "def pearson_corr_score(output1, output2):\n",
        "\txmean = torch.mean(output1)\n",
        "\tymean = torch.mean(output2)\n",
        "\tp_score = torch.sum((output1-xmean)*(output2-ymean))/torch.sqrt(torch.sum((output1-xmean)**2)*torch.sum((output2-ymean)**2))\n",
        "\treturn p_score\n",
        "\n",
        "def weighted_score(output1,output2):\n",
        "\tedist = torch.dist(output1, output2)**2\n",
        "\tcos_sim = cos_sim_score(output1, output2)\n",
        "\tp_corr = pearson_corr_score(output1, output2)\n",
        "\tw_score = 0.2*edist+0.4*cos_sim+0.4*p_corr\n",
        "\treturn [w_score, edist, cos_sim, p_corr]\n",
        "\n",
        "def num_segment(duration, seg):\n",
        "\tinterval = seg/2\n",
        "\tnum = np.ceil((duration-seg)/interval)\n",
        "\treturn num\n",
        "\n",
        "def seg_interval(ind, duration, seg):\n",
        "\tinterval = seg/2\n",
        "\tstart_time = ind*interval\n",
        "\tend_time = start_time+seg\n",
        "\tif end_time>duration:\n",
        "\t\tend_time = duration\n",
        "\treturn start_time, end_time\n",
        "# ---------------------------------------"
      ],
      "id": "MGr7ViGnEbcJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-cSX9B5EbcJ"
      },
      "source": [
        "```main()``` function for training and testing\n",
        "\n"
      ],
      "id": "I-cSX9B5EbcJ"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FPp7HlqeEbcJ"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# sample rate of songs\n",
        "SAMPLE_RATE = 48000\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\t#\n",
        "\tif MODE == 'train':\n",
        "\n",
        "\t\t# Load the training dataset\n",
        "\t\tfolder_dataset = datasets.ImageFolder(root=DATASET_PATH)\n",
        "\n",
        "\t\t# Resize the images and transform to tensors\n",
        "\t\ttransformation = transforms.Compose([transforms.Resize((299,299)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t transforms.ToTensor()\n",
        "\t\t\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "\t\t# Initialize the network\n",
        "\t\tdataset = NetworkDataset(imageFolderDataset=folder_dataset,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\ttransform=transformation)\n",
        "\n",
        "\t\t# Load the training dataset\n",
        "\t\ttrain_dataloader = DataLoader(dataset,\n",
        "\t\t\t\t\t\t\t\tshuffle=True,\n",
        "\t\t\t\t\t\t\t\tnum_workers=8,\n",
        "\t\t\t\t\t\t\t\tbatch_size=16)\n",
        "\n",
        "\t\t# Initialize network\n",
        "\t\tnet = xception(pretrained = True, to_cuda = CUDA, reload_previous = RELOAD_PREVIOUS, RELOAD_PATH = RELOAD_PATH)\n",
        "\t\t# Define the loss metric as Triplet Loss\n",
        "\t\tcriterion = TripletLoss()\n",
        "\t\t# Define the optimizer used as Adam\n",
        "\t\toptimizer = optim.Adam(net.parameters(), lr = 0.001 )\n",
        "\n",
        "\n",
        "\t\t# Record loss value during training\n",
        "\t\tcounter = []\n",
        "\t\tloss_history = []\n",
        "\t\tloss_epoch_history = np.zeros([TRAINING_EPOCH, 2])\n",
        "\t\t# If training is restarted from a savepoint, load previous loss history\n",
        "\t\tif RELOAD_PREVIOUS:\n",
        "\t\t\tdata = np.loadtxt(\"{0}_loss.csv\".format(MODEL_PATH), delimiter=',')\n",
        "\t\t\tnonzerorow, nonzerocol = np.nonzero(data)\n",
        "\t\t\tcutrow = np.min([np.max(nonzerorow), STARTING_EPOCH])\n",
        "\t\t\tnonzerodata = data[:cutrow+1,:]\n",
        "\t\t\tloss_epoch_history = np.vstack([nonzerodata, loss_epoch_history])\n",
        "\t\titeration_number= 0\n",
        "\t\t# Ending index of epoch\n",
        "\t\tending_epoch = STARTING_EPOCH+TRAINING_EPOCH\n",
        "\t\t# Iterate throught the epochs\n",
        "\t\tfor epoch in range(STARTING_EPOCH, ending_epoch):\n",
        "\t\t\t# Initialize loss\n",
        "\t\t\tepoch_loss_ave = 0.0\n",
        "\t\t\t# Iterate over batches\n",
        "\t\t\tfor i, (img0, img1, img2) in enumerate(train_dataloader, 0):\n",
        "\t\t\t\tif CUDA:\n",
        "\t\t\t\t\t# Send the images and labels to CUDA\n",
        "\t\t\t\t\timg0, img1, img2 = img0.cuda(), img1.cuda(), img2.cuda()\n",
        "\n",
        "\t\t\t\t# Zero the gradients\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\t\t# Pass in the two images into the network and obtain two outputs\n",
        "\t\t\t\toutput1, output2, output3 = net(img0, img1, img2)\n",
        "\n",
        "\t\t\t\t# Pass the outputs of the networks and label into the loss function\n",
        "\t\t\t\tloss_triplet = criterion(output1, output2, output3)\n",
        "\n",
        "\t\t\t\t# Calculate the backpropagation\n",
        "\t\t\t\tloss_triplet.backward()\n",
        "\n",
        "\t\t\t\t# Optimize\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\t# Accumulate loss value\n",
        "\t\t\t\tepoch_loss_ave += loss_triplet.item()\n",
        "\n",
        "\t\t\t\t# Every 10 batches print out the loss\n",
        "\t\t\t\tif i % 10 == 0 :\n",
        "\t\t\t\t\tprint(f\"Epoch number {epoch} batch {i}\\n Current loss {loss_triplet.item()}\\n\")\n",
        "\t\t\t\t\titeration_number += 10\n",
        "\t\t\t\t\tcounter.append(iteration_number)\n",
        "\t\t\t\t\tloss_history.append(loss_triplet.item())\n",
        "\t\t\t# Average loss value over batches\n",
        "\t\t\tepoch_loss_ave = epoch_loss_ave/i\n",
        "\t\t\tprint(f\"Epoch number {epoch}\\n Current loss {epoch_loss_ave}\\n\")\n",
        "\t\t\t# Record loss value\n",
        "\t\t\tloss_epoch_history[epoch,0] = epoch\n",
        "\t\t\tloss_epoch_history[epoch,1] = epoch_loss_ave\n",
        "\t\t\t# Save current savepoint\n",
        "\t\t\ttorch.save(net.state_dict(), \"{0}_{1}.pth\".format(MODEL_PATH, epoch))\n",
        "\t\t\t# Save current loss value\n",
        "\t\t\tnp.savetxt(\"{0}_loss.csv\".format(MODEL_PATH), loss_epoch_history, delimiter=',')\n",
        "\n",
        "\telif MODE == 'mean_distance':\n",
        "\n",
        "\t\t# Load the training dataset\n",
        "\t\tfolder_dataset = datasets.ImageFolder(root=DATASET_PATH)\n",
        "\n",
        "\t\t# Resize the images and transform to tensors\n",
        "\t\ttransformation = transforms.Compose([transforms.Resize((299,299)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t transforms.ToTensor()\n",
        "\t\t\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "\t\t # Initialize the network\n",
        "\t\tdataset = NetworkDataset(imageFolderDataset=folder_dataset,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\ttransform=transformation)\n",
        "\n",
        "\t\t# Load the training dataset\n",
        "\t\tvalidate_dataloader = DataLoader(dataset,\n",
        "\t\t\t\t\t\t\t\tshuffle=False,\n",
        "\t\t\t\t\t\t\t\tnum_workers=8,\n",
        "\t\t\t\t\t\t\t\tbatch_size=128)\n",
        "\n",
        "\t\t# Initialize the network\n",
        "\t\tnet = xception(pretrained = True, to_cuda = CUDA, reload_previous = RELOAD_PREVIOUS, RELOAD_PATH = RELOAD_PATH)\n",
        "\t\t# Define evaluation metric as mean distance\n",
        "\t\tcriterion = MeanDistance()\n",
        "\n",
        "\t\t# Record loss value during training\n",
        "\t\tending_epoch = STARTING_EPOCH+TRAINING_EPOCH\n",
        "\t\t# Initialize array to record distance history\n",
        "\t\tdistance_history = np.zeros([ending_epoch,3])\n",
        "\t\t# Iterate throught the epochs\n",
        "\t\tfor epoch in range(STARTING_EPOCH, ending_epoch):\n",
        "\t\t\t# Load savepoint at each epoch\n",
        "\t\t\tLOAD_PATH = \"{1}_{0}.pth\".format(epoch, MODEL_PATH)\n",
        "\t\t\tload_model(net, path = LOAD_PATH)\n",
        "\t\t\t# Initialize mean distance\n",
        "\t\t\tepoch_distance_n = 0.0\n",
        "\t\t\tepoch_distance_p = 0.0\n",
        "\t\t\t# Iterate over batches\n",
        "\t\t\tfor i, (img0, img1, img2) in enumerate(validate_dataloader, 0):\n",
        "\n",
        "\t\t\t\tif CUDA:\n",
        "\t\t\t\t\t# Send the images and labels to CUDA\n",
        "\t\t\t\t\timg0, img1, img2 = img0.cuda(), img1.cuda(), img2.cuda()\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\t# Pass in the two images into the network and obtain two outputs\n",
        "\t\t\t\t\toutput1, output2, output3 = net(img0, img1, img2)\n",
        "\n",
        "\t\t\t\t# Pass the outputs of the networks and label into the loss function\n",
        "\t\t\t\tmean_distance_p, mean_distance_n = criterion(output1, output2, output3)\n",
        "\t\t\t\t# Accumulate mean distance\n",
        "\t\t\t\tepoch_distance_n += mean_distance_n.item()\n",
        "\t\t\t\tepoch_distance_p += mean_distance_p.item()\n",
        "\t\t\t# Average distance over batches\n",
        "\t\t\tepoch_distance_n = epoch_distance_n/i\n",
        "\t\t\tepoch_distance_p = epoch_distance_p/i\n",
        "\t\t\tprint(f\"Epoch number {epoch}\\n Mean positive distance {epoch_distance_p}\\n Mean negative distance {epoch_distance_n}\\n\")\n",
        "\t\t\t# Record the mean distances\n",
        "\t\t\tdistance_history[epoch,1] = epoch_distance_p\n",
        "\t\t\tdistance_history[epoch,2] = epoch_distance_n\n",
        "\t\t\tdistance_history[epoch,0] = epoch\n",
        "\t\t\tnp.savetxt(\"{0}_distance.csv\".format(MODEL_PATH), distance_history, delimiter=',')\n",
        "\n",
        "\telif MODE == 'validate':\n",
        "\t\t# Load the training dataset\n",
        "\t\tfolder_dataset = datasets.ImageFolder(root=VALIDATE_PATH)\n",
        "\n",
        "\t\t# Resize the images and transform to tensors\n",
        "\t\ttransformation = transforms.Compose([transforms.Resize((299,299)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t transforms.ToTensor()\n",
        "\t\t\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "\t\t# Initialize the network\n",
        "\t\tdataset = NetworkDataset(imageFolderDataset=folder_dataset,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\ttransform=transformation)\n",
        "\n",
        "\t\t# Load the training dataset\n",
        "\t\tvalidate_dataloader = DataLoader(dataset,\n",
        "\t\t\t\t\t\t\t\tshuffle=False,\n",
        "\t\t\t\t\t\t\t\tnum_workers=8,\n",
        "\t\t\t\t\t\t\t\tbatch_size=128)\n",
        "\t\t# Initialize the network\n",
        "\t\tnet = xception(pretrained = True, to_cuda = CUDA, reload_previous = RELOAD_PREVIOUS, RELOAD_PATH = RELOAD_PATH)\n",
        "\t\t# Define the loss metric as Triplet Loss\n",
        "\t\tcriterion = TripletLoss()\n",
        "\n",
        "\t\tending_epoch = STARTING_EPOCH+TRAINING_EPOCH\n",
        "\n",
        "\t\tvalidate_history = np.zeros([ending_epoch,2])\n",
        "\t\t# Iterate throught the epochs\n",
        "\t\tfor epoch in range(STARTING_EPOCH, ending_epoch):\n",
        "\t\t\tLOAD_PATH = \"{1}_{0}.pth\".format(epoch, MODEL_PATH)\n",
        "\t\t\tload_model(net, path = LOAD_PATH)\n",
        "\t\t\tepoch_loss_ave = 0.0\n",
        "\t\t\t# Iterate over batches\n",
        "\t\t\tfor i, (img0, img1, img2) in enumerate(validate_dataloader, 0):\n",
        "\t\t\t\t# def closure():\n",
        "\t\t\t\t\t# loss = criterion(output1, output2, output3)\n",
        "\t\t\t\t\t# loss.backward()\n",
        "\t\t\t\t\t# return loss\n",
        "\t\t\t\tif CUDA:\n",
        "\t\t\t\t\t# Send the images and labels to CUDA\n",
        "\t\t\t\t\timg0, img1, img2 = img0.cuda(), img1.cuda(), img2.cuda()\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\t# Pass in the two images into the network and obtain two outputs\n",
        "\t\t\t\t\toutput1, output2, output3 = net(img0, img1, img2)\n",
        "\n",
        "\t\t\t\t# Pass the outputs of the networks and label into the loss function\n",
        "\t\t\t\tloss_triplet = criterion(output1, output2, output3)\n",
        "\n",
        "\t\t\t\tepoch_loss_ave += loss_triplet.item()\n",
        "\t\t\tepoch_loss_ave = epoch_loss_ave/i\n",
        "\t\t\tprint(f\"Epoch number {epoch}\\n Validation loss {epoch_loss_ave}\\n\")\n",
        "\t\t\tvalidate_history[epoch,1] = epoch_loss_ave\n",
        "\t\t\tvalidate_history[epoch,0] = epoch\n",
        "\t\t\tnp.savetxt(\"{0}_loss_val.csv\".format(MODEL_PATH), validate_history, delimiter=',')\n",
        "\n",
        "\telif MODE == 'test':\n",
        "\n",
        "\t\t# Initialize the network\n",
        "\t\tnet = xception(pretrained = True, to_cuda = CUDA, reload_previous = RELOAD_PREVIOUS, RELOAD_PATH = RELOAD_PATH)\n",
        "\t\t# Resize the images and transform to tensors\n",
        "\t\ttransformation = transforms.Compose([transforms.Resize((299,299)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t transforms.ToTensor()\n",
        "\t\t\t\t\t\t\t\t\t\t\t])\n",
        "\t\t# Define model used\n",
        "\t\tif 'Melspectrogram' in MODEL_PATH:\n",
        "\t\t\tmel = True\n",
        "\t\t\tinputname = 'melspectrogram'\n",
        "\t\telif 'Chroma' in MODEL_PATH:\n",
        "\t\t\tmel = False\n",
        "\t\t\tinputname = 'chroma'\n",
        "\t\tif '10s' in MODEL_PATH:\n",
        "\t\t\tseg10s = True\n",
        "\t\t\tsegname = '10s'\n",
        "\t\telif 'preprocess' in MODEL_PATH:\n",
        "\t\t\tseg10s = False\n",
        "\t\t\tsegname = 'preprocessed'\n",
        "\n",
        "\t\t# Grab audios\n",
        "\t\tfile_list = os.listdir(TEST_PATH)\n",
        "\n",
        "\t\t# Initialize lists\n",
        "\t\tsong_list = []\n",
        "\t\tduration_list = []\n",
        "\t\tseg_list = []\n",
        "\t\tnum_seg_list = []\n",
        "\t\tfeature_list = []\n",
        "\n",
        "\t\t# Define batch size to run at once\n",
        "\t\trun_batch = 48\n",
        "\n",
        "\t\tfor i in range(len(file_list)):\n",
        "\t\t\t# Find all the mp3 files in the TEST_PATH folder\n",
        "\t\t\tif 'mp3' in file_list[i]:\n",
        "\t\t\t\tfilename = file_list[i].split('.')[0]\n",
        "\t\t\t\t# Get the song name\n",
        "\t\t\t\tsong_list.append(filename)\n",
        "\t\t\t\t# Load the song\n",
        "\t\t\t\ty, sr = librosa.load(os.path.join(TEST_PATH, file_list[i]), sr=SAMPLE_RATE)\n",
        "\t\t\t\t# Select the slicing length\n",
        "\t\t\t\tif seg10s:\n",
        "\t\t\t\t\tseg = 10\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tseg = calculate_eightbars_duration(y, SAMPLE_RATE)\n",
        "\t\t\t\tseg_list.append(seg)\n",
        "\n",
        "\t\t\t\t# Get the duration of the song\n",
        "\t\t\t\tduration = librosa.get_duration(y=y, sr=SAMPLE_RATE)\n",
        "\t\t\t\tduration_list.append(duration)\n",
        "\n",
        "\t\t\t\t# Compute how many segments could be sliced out\n",
        "\t\t\t\tnumseg = num_segment(duration, seg)\n",
        "\t\t\t\tnum_seg_list.append(numseg)\n",
        "\n",
        "\t\t\t\t# Initialize tensor to save transformed diagrams and outputted feature maps\n",
        "\t\t\t\tsegarrs = torch.empty((int(numseg), 3, 299, 299))\n",
        "\t\t\t\tfeaturemap = torch.empty((int(numseg), 1000))\n",
        "\n",
        "\t\t\t\t# Iterate over the segments\n",
        "\t\t\t\tfor j in range(int(numseg)):\n",
        "\t\t\t\t\t# Define the slice starting and ending time\n",
        "\t\t\t\t\tstart_time, end_time = seg_interval(j, duration, seg)\n",
        "\t\t\t\t\tstart_sample = librosa.time_to_samples(start_time, sr=SAMPLE_RATE)\n",
        "\t\t\t\t\tend_sample = librosa.time_to_samples(end_time, sr=SAMPLE_RATE)\n",
        "\t\t\t\t\t# Slice the audio\n",
        "\t\t\t\t\taudio_clip = y[start_sample:end_sample]\n",
        "\t\t\t\t\t# Generate appropraite diagrams\n",
        "\t\t\t\t\tif mel:\n",
        "\t\t\t\t\t\timg = get_mel_spectrogram(audio_clip)\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\timg = get_chroma_feature(audio_clip)\n",
        "\n",
        "\t\t\t\t\t# Transform the diagrams and save them\n",
        "\t\t\t\t\tarr = transformation(img)\n",
        "\t\t\t\t\tsegarrs[j,:,:,:] = arr\n",
        "\n",
        "\t\t\t\t\t# Compute feature maps over batches\n",
        "\t\t\t\t\tif (j%run_batch == 0 and j!=0):\n",
        "\t\t\t\t\t\tbatch_ind = j//run_batch\n",
        "\t\t\t\t\t\t# Compute batch starting and ending index in saved input array\n",
        "\t\t\t\t\t\tstart_ind = (batch_ind-1)*run_batch\n",
        "\t\t\t\t\t\tend_ind = np.min([(batch_ind)*run_batch,int(numseg)])\n",
        "\t\t\t\t\t\t# Select the batch inputs\n",
        "\t\t\t\t\t\tsegbatch = segarrs[start_ind:end_ind,:,:,:]\n",
        "\t\t\t\t\t\tif CUDA:\n",
        "\t\t\t\t\t\t\tsegbatch = segbatch.cuda()\n",
        "\t\t\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\t\t\t# Compute the feature map\n",
        "\t\t\t\t\t\t\tfeaturemap_batch = net.forward_once(segbatch)\n",
        "\t\t\t\t\t\t# Save the feature map\n",
        "\t\t\t\t\t\tfeaturemap[start_ind:end_ind,:] = featuremap_batch.cpu()\n",
        "\n",
        "\t\t\t\t\tif (j==int(numseg)-1):\n",
        "\t\t\t\t\t\tbatch_ind = j//run_batch\n",
        "\t\t\t\t\t\t# Compute batch starting and ending index in saved input array\n",
        "\t\t\t\t\t\tstart_ind = (batch_ind)*run_batch\n",
        "\t\t\t\t\t\tend_ind = int(numseg)\n",
        "\t\t\t\t\t\t# Select the batch inputs\n",
        "\t\t\t\t\t\tsegbatch = segarrs[start_ind:end_ind,:,:,:]\n",
        "\t\t\t\t\t\tif CUDA:\n",
        "\t\t\t\t\t\t\tsegbatch = segbatch.cuda()\n",
        "\t\t\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\t\t\t# Compute the feature map\n",
        "\t\t\t\t\t\t\tfeaturemap_batch = net.forward_once(segbatch)\n",
        "\t\t\t\t\t\t# Save the feature map\n",
        "\t\t\t\t\t\tfeaturemap[start_ind:end_ind,:] = featuremap_batch.cpu()\n",
        "\t\t\t\t# Save the computed feature maps\n",
        "\t\t\t\tfeature_list.append(featuremap)\n",
        "\n",
        "\t\t# Initialize array to save similarity scores\n",
        "\t\trecord = np.zeros([int(num_seg_list[0]*num_seg_list[1]),8])\n",
        "\t\tcounter = 0\n",
        "\t\t# Get the duration, segment length and computed feature maps of the pair\n",
        "\t\tduration1 = duration_list[0]\n",
        "\t\tduration2 = duration_list[1]\n",
        "\t\tseg1 = seg_list[0]\n",
        "\t\tseg2 = seg_list[1]\n",
        "\t\tfeaturemap1 = feature_list[0]\n",
        "\t\tfeaturemap2 = feature_list[1]\n",
        "\t\t# Iterate over feature maps of all segments\n",
        "\t\tfor i in range(int(num_seg_list[0])):\n",
        "\t\t\tout1 = featuremap1[[i],:]\n",
        "\t\t\t# Compute the corresponding segment starting and ending time\n",
        "\t\t\tstart_time1, end_time1 = seg_interval(i, duration1, seg1)\n",
        "\t\t\tfor j in range(int(num_seg_list[1])):\n",
        "\t\t\t\tout2 = featuremap2[[j],:]\n",
        "\t\t\t\t# Compute the corresponding segment starting and ending time\n",
        "\t\t\t\tstart_time2, end_time2 = seg_interval(j, duration2, seg2)\n",
        "\t\t\t\t# Compute similarity scores of pairs\n",
        "\t\t\t\t[w_score, edist, cos_sim, p_corr] = weighted_score(out1, out2)\n",
        "\t\t\t\t# Record the computed results\n",
        "\t\t\t\trecord[counter, 0] = start_time1\n",
        "\t\t\t\trecord[counter, 1] = end_time1\n",
        "\t\t\t\trecord[counter, 2] = start_time2\n",
        "\t\t\t\trecord[counter, 3] = end_time2\n",
        "\t\t\t\trecord[counter, 4] = w_score\n",
        "\t\t\t\trecord[counter, 5] = edist\n",
        "\t\t\t\trecord[counter, 6] = cos_sim\n",
        "\t\t\t\trecord[counter, 7] = p_corr\n",
        "\t\t\t\tcounter += 1\n",
        "\n",
        "\t\t# Find the minimum euclidean distance and corresponding index\n",
        "\t\tmin_score = np.min(record[:,5])\n",
        "\t\tmin_index = np.argmin(record[:,5])\n",
        "\n",
        "\t\tprint(\"For Case {9}, using {0} and {1} segments, the min euclidean distance is {2}. \\n Occurs at {3} ({4}:{5}) and {6} ({7}:{8}). \\n\".format(inputname,segname, min_score, song_list[0], record[min_index,0], record[min_index,1], song_list[1], record[min_index,2], record[min_index,3],TEST_PATH.split(' ')[-1]))\n",
        "\t\t# Save records\n",
        "\t\trecordname = \"{0}_{1}_{2}_{3}\".format(inputname, segname, song_list[0], song_list[1])\n",
        "\t\tnp.savetxt(\"{1}/{0}.csv\".format(recordname,TEST_PATH),np.array(record), delimiter=',')"
      ],
      "id": "FPp7HlqeEbcJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "One caveat is that if you let the notebook run in the background without interaction, google colab will kill the session after 12 hours. To avoid the issue, we need to set a clicker in the background that will click on the connect button in a set time interval. To do that, do Ctrl-Shift-i to access the web console, the copy the following code to the console:\n",
        "\n",
        "```\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "}\n",
        "var clicker = setInterval(ClickConnect,60000);\n",
        "```\n",
        "\n",
        "To clear the clicker, do:\n",
        "```\n",
        "clearInterval(clicker);\n",
        "```"
      ],
      "metadata": {
        "id": "m-4BN8G9cGf8"
      },
      "id": "m-4BN8G9cGf8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we run the ```main()``` function, which will run and save models based on the path you set below. Remember to change ```DATASET_PATH``` to the path pointing to the dataset folder, change ```MODEL_PATH``` to your desired folder for saving the intermittent model parameters."
      ],
      "metadata": {
        "id": "BkPFzYtzdWxR"
      },
      "id": "BkPFzYtzdWxR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are four different modes to choose from:\n",
        "\n",
        "1.   ```'train'```: start training the model using Adam optimizer and Triplet loss. If ```RELOAD_PREVIOUS``` is set to False, ```STARTING_EPOCH``` will be set to 0, and the model will be trained from scratch.\n",
        "If the user wants to start training from an existing savepoint, set ```RELOAD_PREVIOUS``` to ```True``` and there will be a prompt asking for the starting epoch of the training. The user should input the epoch corresponding to the savepoint.          \n",
        "2.   ```'validate'```: the script will reload savepoints in ```MODEL_PATH``` and compute the loss value of the validation dataset for each savepoint.\n",
        "3.   ```'mean_distance'```: the script will reload savepoints in ```MODEL_PATH``` and compute the mean distance value of the training dataset for each savepoint.\n",
        "4.   ```'test'```: the script to load the pair of mp3 files in ```TEST_PATH```. By reloading the savepoint in ```RELOAD_PATH```, the model will slice segments of the two mp3 files in ```TEST_PATH``` based on the ```model_name``` and compute the feature maps of the segments. A minimum Euclidean distance between feature maps and the corresponding segments are printed to display."
      ],
      "metadata": {
        "id": "F8nt35nttG9Z"
      },
      "id": "F8nt35nttG9Z"
    },
    {
      "cell_type": "code",
      "source": [
        "MODE = 'train'\n",
        "if MODE == 'test' or MODE == 'validate' or MODE == 'mean_distance':\n",
        "\tRELOAD_PREVIOUS = True\n",
        "else:\n",
        "\tresponse = input(\"Do you want to reload a savepoint for retraining? (Yes or No) \")\n",
        "\tRELOAD_PREVIOUS = True if response == 'Yes' else False\n",
        "\n",
        "# path pointing to the training dataset\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Music_Plagiarism/chroma_feature10s_dataset/training\"\n",
        "# path pointing to the validation dataset\n",
        "VALIDATE_PATH = \"/content/drive/MyDrive/Music_Plagiarism/chroma_feature10s_dataset/testing\"\n",
        "# path pointing to the folder containing a pair of mp3 files to be compared\n",
        "# the folder should ONLY contain two mp3 files\n",
        "TEST_PATH = \"/content/drive/MyDrive/Music_Plagiarism/Dataset/Audio/Case 1\"\n",
        "# path pointing to the folder for saving training savepoints\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Music_Plagiarism/chroma_feature10s_savepoint\"\n",
        "# name for saving your model\n",
        "model_name = \"model_chroma10s\"\n",
        "# controls the script whether or not to use GPU resources.\n",
        "CUDA = True\n",
        "# epoch duration of the training\n",
        "TRAINING_EPOCH = 150\n",
        "if RELOAD_PREVIOUS:\n",
        "\tif MODE == 'train':\n",
        "\t\tSTARTING_EPOCH = int(input(\"Enter the starting epoch for retraining: \"))\n",
        "\t\tRELOAD_PATH = \"{1}/{2}_{0}.pth\".format(STARTING_EPOCH, MODEL_PATH, model_name)\n",
        "\telse:\n",
        "\t\tSTARTING_EPOCH = 0\n",
        "\t\tRELOAD_PATH = input(\"Enter the path to the savepoint: \")\n",
        "else:\n",
        "\tSTARTING_EPOCH = 0\n",
        "\tRELOAD_PATH = \"\"\n",
        "main()"
      ],
      "metadata": {
        "id": "WgRo3Hp4bP-t"
      },
      "id": "WgRo3Hp4bP-t",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "3de59ade2ee4eabf8d6554090db31bdd94608df00f05391c2d316a7da62ee3f6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}