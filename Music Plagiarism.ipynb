{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start loading some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment \n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import mutagen\n",
    "import os\n",
    "import pandas as pd\n",
    "from pytube import YouTube\n",
    "from youtubesearchpython import VideosSearch\n",
    "from moviepy.editor import *\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load some pytorch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the xception model that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \r\n",
    "Creates an Xception Model as defined in:\r\n",
    "\r\n",
    "Francois Chollet\r\n",
    "Xception: Deep Learning with Depthwise Separable Convolutions\r\n",
    "https://arxiv.org/pdf/1610.02357.pdf\r\n",
    "\r\n",
    "This weights ported from the Keras implementation. Achieves the following performance on the validation set:\r\n",
    "\r\n",
    "Loss:0.9173 Prec@1:78.892 Prec@5:94.292\r\n",
    "\r\n",
    "REMEMBER to set your image size to 3x299x299 for both test and validation\r\n",
    "\r\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\r\n",
    "                                  std=[0.5, 0.5, 0.5])\r\n",
    "\r\n",
    "The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\r\n",
    "\"\"\"\r\n",
    "import math\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.utils.model_zoo as model_zoo\r\n",
    "from torch.nn import init\r\n",
    "import torch\r\n",
    "\r\n",
    "__all__ = ['xception']\r\n",
    "\r\n",
    "model_urls = {\r\n",
    "#     'xception':'https://www.dropbox.com/s/1hplpzet9d7dv29/xception-c0a72b38.pth.tar?dl=1'\r\n",
    "    'xception':'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth'\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "class SeparableConv2d(nn.Module):\r\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\r\n",
    "        super(SeparableConv2d,self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\r\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\r\n",
    "    \r\n",
    "    def forward(self,x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = self.pointwise(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "class Block(nn.Module):\r\n",
    "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\r\n",
    "        super(Block, self).__init__()\r\n",
    "\r\n",
    "        if out_filters != in_filters or strides!=1:\r\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\r\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\r\n",
    "        else:\r\n",
    "            self.skip=None\r\n",
    "        \r\n",
    "        self.relu = nn.ReLU(inplace=True)\r\n",
    "        rep=[]\r\n",
    "\r\n",
    "        filters=in_filters\r\n",
    "        if grow_first:\r\n",
    "            rep.append(self.relu)\r\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\r\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\r\n",
    "            filters = out_filters\r\n",
    "\r\n",
    "        for i in range(reps-1):\r\n",
    "            rep.append(self.relu)\r\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\r\n",
    "            rep.append(nn.BatchNorm2d(filters))\r\n",
    "        \r\n",
    "        if not grow_first:\r\n",
    "            rep.append(self.relu)\r\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\r\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\r\n",
    "\r\n",
    "        if not start_with_relu:\r\n",
    "            rep = rep[1:]\r\n",
    "        else:\r\n",
    "            rep[0] = nn.ReLU(inplace=False)\r\n",
    "\r\n",
    "        if strides != 1:\r\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\r\n",
    "        self.rep = nn.Sequential(*rep)\r\n",
    "\r\n",
    "    def forward(self,inp):\r\n",
    "        x = self.rep(inp)\r\n",
    "\r\n",
    "        if self.skip is not None:\r\n",
    "            skip = self.skip(inp)\r\n",
    "            skip = self.skipbn(skip)\r\n",
    "        else:\r\n",
    "            skip = inp\r\n",
    "\r\n",
    "        x+=skip\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class Xception(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Xception optimized for the ImageNet dataset, as specified in\r\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, num_classes=1000):\r\n",
    "        \"\"\" Constructor\r\n",
    "        Args:\r\n",
    "            num_classes: number of classes\r\n",
    "        \"\"\"\r\n",
    "        super(Xception, self).__init__()\r\n",
    "\r\n",
    "        \r\n",
    "        self.num_classes = num_classes\r\n",
    "\r\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\r\n",
    "        self.bn1 = nn.BatchNorm2d(32)\r\n",
    "        self.relu = nn.ReLU(inplace=True)\r\n",
    "\r\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\r\n",
    "        self.bn2 = nn.BatchNorm2d(64)\r\n",
    "        #do relu here\r\n",
    "\r\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\r\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\r\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\r\n",
    "\r\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "\r\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\r\n",
    "\r\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\r\n",
    "\r\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\r\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\r\n",
    "\r\n",
    "        #do relu here\r\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\r\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\r\n",
    "\r\n",
    "        self.fc = nn.Linear(2048, num_classes)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        #------- init weights --------\r\n",
    "        for m in self.modules():\r\n",
    "            if isinstance(m, nn.Conv2d):\r\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n",
    "            elif isinstance(m, nn.BatchNorm2d):\r\n",
    "                m.weight.data.fill_(1)\r\n",
    "                m.bias.data.zero_()\r\n",
    "        #-----------------------------\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    def forward_once(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = self.bn1(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        \r\n",
    "        x = self.conv2(x)\r\n",
    "        x = self.bn2(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        \r\n",
    "        x = self.block1(x)\r\n",
    "        x = self.block2(x)\r\n",
    "        x = self.block3(x)\r\n",
    "        x = self.block4(x)\r\n",
    "        x = self.block5(x)\r\n",
    "        x = self.block6(x)\r\n",
    "        x = self.block7(x)\r\n",
    "        x = self.block8(x)\r\n",
    "        x = self.block9(x)\r\n",
    "        x = self.block10(x)\r\n",
    "        x = self.block11(x)\r\n",
    "        x = self.block12(x)\r\n",
    "        \r\n",
    "        x = self.conv3(x)\r\n",
    "        x = self.bn3(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        \r\n",
    "        x = self.conv4(x)\r\n",
    "        x = self.bn4(x)\r\n",
    "        x = self.relu(x)\r\n",
    "\r\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\r\n",
    "        x = x.view(x.size(0), -1)\r\n",
    "        x = self.fc(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "    def forward(self, input1, input2, input3):\r\n",
    "        # In this function we pass in both images and obtain both vectors\r\n",
    "        # which are returned\r\n",
    "        output1 = self.forward_once(input1)\r\n",
    "        output2 = self.forward_once(input2)\r\n",
    "        output3 = self.forward_once(input3)\r\n",
    "\r\n",
    "        return output1, output2, output3\r\n",
    "\r\n",
    "# Define the triplet Loss Function\r\n",
    "class TripletLoss(torch.nn.Module):\r\n",
    "    def __init__(self, margin=2.0):\r\n",
    "        super(TripletLoss, self).__init__()\r\n",
    "        self.margin = margin\r\n",
    "\r\n",
    "    # output 1 for anchor, output2 for positive, output3 for negative\r\n",
    "    def forward(self, output1, output2, output3):\r\n",
    "      # Calculate the euclidean distance and calculate the contrastive loss\r\n",
    "      euclidean_distance_p = F.pairwise_distance(output1, output2, keepdim = True)\r\n",
    "      euclidean_distance_n = F.pairwise_distance(output1, output3, keepdim = True)\r\n",
    "\r\n",
    "      loss_triple = torch.mean((torch.clamp(self.margin - euclidean_distance_n + euclidean_distance_p, min=0.0)))\r\n",
    "\r\n",
    "      return loss_triple\r\n",
    "\r\n",
    "def xception(pretrained=True, to_cuda = False, reload_previous = False, **kwargs):\r\n",
    "    \"\"\"\r\n",
    "    Construct Xception.\r\n",
    "    \"\"\"\r\n",
    "    if to_cuda:\r\n",
    "        model = Xception(**kwargs).cuda()\r\n",
    "    else:\r\n",
    "        model = Xception(**kwargs)\r\n",
    "    if pretrained:\r\n",
    "        model.load_state_dict(torch.load('xception-43020ad28.pth'))\r\n",
    "        # model.load_state_dict(model_zoo.load_url(model_urls['xception']))\r\n",
    "    if reload_previous:\r\n",
    "        load_model(model)\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def save_model(model, path = './xception.pth'):\r\n",
    "    torch.save(model.state_dict(), path)\r\n",
    "\r\n",
    "def load_model(model, path = './xception.pth'):\r\n",
    "    model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\r\n",
    "    def __init__(self,imageFolderDataset,transform=None):\r\n",
    "        self.imageFolderDataset = imageFolderDataset    \r\n",
    "        self.transform = transform\r\n",
    "        \r\n",
    "    def __getitem__(self,index):\r\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\r\n",
    "\r\n",
    "        #We need to get a image in the same class and in a different class\r\n",
    "        while True:\r\n",
    "            #Look untill the same class image is found\r\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs) \r\n",
    "            if img0_tuple[1] == img1_tuple[1]:\r\n",
    "                break\r\n",
    "        while True:\r\n",
    "            #Look untill a different class image is found\r\n",
    "            img2_tuple = random.choice(self.imageFolderDataset.imgs) \r\n",
    "            if img0_tuple[1] != img2_tuple[1]:\r\n",
    "                break\r\n",
    "\r\n",
    "        # open image file\r\n",
    "        img0 = Image.open(img0_tuple[0])\r\n",
    "        img1 = Image.open(img1_tuple[0])\r\n",
    "        img2 = Image.open(img2_tuple[0])\r\n",
    "\r\n",
    "        # to grayscale\r\n",
    "        # img0 = img0.convert(\"L\")\r\n",
    "        # img1 = img1.convert(\"L\")\r\n",
    "        # img2 = img2.convert(\"L\")\r\n",
    "\r\n",
    "        # to RBG\r\n",
    "        img0 = img0.convert(\"RGB\")\r\n",
    "        img1 = img1.convert(\"RGB\")\r\n",
    "        img2 = img2.convert(\"RGB\")\r\n",
    "\r\n",
    "\r\n",
    "        if self.transform is not None:\r\n",
    "            img0 = self.transform(img0)\r\n",
    "            img1 = self.transform(img1)\r\n",
    "            img2 = self.transform(img2)\r\n",
    "        \r\n",
    "        return img0, img1, img2\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ string related\r\n",
    "def convert_time_to_sec(time_str):\r\n",
    "    # convert time string like ['1:27'] to seconds\r\n",
    "    if \",\" in time_str:\r\n",
    "        return [convert_time_to_sec(i) for i in time_str.split(\",\")]\r\n",
    "                \r\n",
    "    # if \"[\" in time_str:\r\n",
    "    time_str = time_str.replace(\"[\", \"\")\r\n",
    "    time_str = time_str.replace(\"]\", \"\")\r\n",
    "    time_str = time_str.replace(\"'\", \"\")\r\n",
    "\r\n",
    "    if \":\" in time_str:\r\n",
    "        time_str = time_str.split(\":\")\r\n",
    "    elif \"_\" in time_str:\r\n",
    "        time_str = time_str.split(\"_\")\r\n",
    "    return int(time_str[0]) * 60 + int(time_str[1])\r\n",
    "\r\n",
    "def replace_invalid_char(string):\r\n",
    "\tinvalid_char = ['/',':','*','?','\"','<','>','|']\r\n",
    "\tnew_string = ''\r\n",
    "\tfor s in string:\r\n",
    "\t\tif s in invalid_char:\r\n",
    "\t\t\ts = '_'\r\n",
    "\t\tnew_string += s\r\n",
    "\treturn new_string\r\n",
    "\r\n",
    "def remove_special_char(string):\r\n",
    "\ttest_str = ''.join(letter for letter in string if letter.isalnum())\r\n",
    "\treturn test_str\r\n",
    "#--------------------------------------------\r\n",
    "\r\n",
    "\r\n",
    "# ------------------------- audio related\r\n",
    "def stretch_audio(audio, sr, rate):\r\n",
    "\tnew_audio = librosa.effects.time_stretch(audio, rate=rate)\r\n",
    "\treturn new_audio\r\n",
    "\r\n",
    "def shift_audio(audio, sr, semitone):\r\n",
    "\ty_shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=semitone)\r\n",
    "\treturn y_shifted\r\n",
    "\r\n",
    "def bpm_estimation(audio, sr):\r\n",
    "\tbpm, _ = librosa.beat.beat_track(y=audio, sr=sr)\r\n",
    "\treturn bpm\r\n",
    "\r\n",
    "def calculate_eightbars_duration(audio, sr):\r\n",
    "\tbpm = bpm_estimation(audio, sr)\r\n",
    "\t# assuming a 44 time signature\r\n",
    "\tsecs = 60/bpm*4*8\r\n",
    "\treturn secs\r\n",
    "\r\n",
    "# --------------------------------------\r\n",
    "\r\n",
    "# ---------------------for dataset conversion\r\n",
    "def list_of_existing_sampletime(dir_list):\r\n",
    "    return np.array(dir_list, dtype = np.int32)\r\n",
    "\r\n",
    "def find_closest_number(array, target):\r\n",
    "  # Find the minimum distance between the target value and each number in the array.\r\n",
    "  distances = [np.abs(target - number) for number in array]\r\n",
    "  # Return the index of the number with the minimum distance.\r\n",
    "  return distances.index(min(distances))\r\n",
    "#--------------------------------------------\r\n",
    "\r\n",
    "# ------------------------ for images\r\n",
    "# Showing images\r\n",
    "def imshow(img, text=None):\r\n",
    "\tnpimg = img.numpy()\r\n",
    "\tplt.axis(\"off\")\r\n",
    "\tif text:\r\n",
    "\t\tplt.text(75, 8, text, style='italic',fontweight='bold',\r\n",
    "\t\t\tbbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\r\n",
    "\t\t\r\n",
    "\tplt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n",
    "\tplt.show()    \r\n",
    "\r\n",
    "# Plotting data\r\n",
    "def show_plot(iteration,loss):\r\n",
    "\tplt.plot(iteration,loss)\r\n",
    "\tplt.show()\r\n",
    "#-------------------------------------------\r\n",
    "\r\n",
    "# -------------- similarity score functions\r\n",
    "def cos_sim_score(output1, output2):\r\n",
    "\tscore = F.cosine_similarity(output1,output2, dim = 1)\r\n",
    "\treturn score \r\n",
    "\r\n",
    "def pearson_corr_score(output1, output2):\r\n",
    "\txmean = torch.mean(output1)\r\n",
    "\tymean = torch.mean(output2)\r\n",
    "\tp_score = torch.sum((output1-xmean)*(output2-ymean))/torch.sqrt(torch.sum((output1-xmean)**2)*torch.sum((output2-ymean)**2))\r\n",
    "\treturn p_score\r\n",
    "\r\n",
    "def weighted_score(output1,output2):\r\n",
    "\tw_score = 0.2*F.pairwise_distance(output1, output2, keepdim = True)**2+0.4*cos_sim_score(output1, output2)+0.4*pearson_corr_score(output1,output2)\r\n",
    "\treturn w_score\r\n",
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./melspectrogram10s_dataset/training/\"\r\n",
    "MODEL_PATH = \"./model.pth\"\r\n",
    "TRAINING_EPOCH = 150\r\n",
    "RELOAD_PREVIOUS = False\r\n",
    "MODE = 'train'\r\n",
    "\r\n",
    "def main():\r\n",
    "    if MODE == 'train':\r\n",
    "\r\n",
    "        # Load the training dataset\r\n",
    "        folder_dataset = datasets.ImageFolder(root=DATASET_PATH)\r\n",
    "\r\n",
    "        # Resize the images and transform to tensors\r\n",
    "        transformation = transforms.Compose([transforms.Resize((299,299)),\r\n",
    "                                             transforms.ToTensor()\r\n",
    "                                            ])\r\n",
    "\r\n",
    "        # Initialize the network\r\n",
    "        dataset = NetworkDataset(imageFolderDataset=folder_dataset,\r\n",
    "                                                transform=transformation)\r\n",
    "\r\n",
    "        # Load the training dataset\r\n",
    "        train_dataloader = DataLoader(dataset,\r\n",
    "                                shuffle=True,\r\n",
    "                                num_workers=8,\r\n",
    "                                batch_size=32)\r\n",
    "\r\n",
    "\r\n",
    "        net = xception(pretrained = True, to_cuda = True, reload_previous = RELOAD_PREVIOUS)\r\n",
    "        criterion = TripletLoss()\r\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.0005 )\r\n",
    "\r\n",
    "        # base_optimizer = torch.optim.Adam  # define an optimizer for the \"sharpness-aware\" update\r\n",
    "        # optimizer = SAM(net.parameters(), base_optimizer, lr=0.1)\r\n",
    "\r\n",
    "        counter = []\r\n",
    "        loss_history = [] \r\n",
    "        iteration_number= 0\r\n",
    "\r\n",
    "        # Iterate throught the epochs\r\n",
    "        for epoch in range(TRAINING_EPOCH):\r\n",
    "\r\n",
    "            # Iterate over batches\r\n",
    "            for i, (img0, img1, img2) in enumerate(train_dataloader, 0):\r\n",
    "                # def closure():\r\n",
    "                    # loss = criterion(output1, output2, output3)\r\n",
    "                    # loss.backward()\r\n",
    "                    # return loss\r\n",
    "                if torch.cuda.is_available():\r\n",
    "                    # Send the images and labels to CUDA\r\n",
    "                    img0, img1, img2 = img0.cuda(), img1.cuda(), img2.cuda()\r\n",
    "\r\n",
    "                # Zero the gradients\r\n",
    "                optimizer.zero_grad()\r\n",
    "\r\n",
    "                # Pass in the two images into the network and obtain two outputs\r\n",
    "                output1, output2, output3 = net(img0, img1, img2)\r\n",
    "\r\n",
    "                # Pass the outputs of the networks and label into the loss function\r\n",
    "                loss_triplet = criterion(output1, output2, output3)\r\n",
    "\r\n",
    "                # Calculate the backpropagation\r\n",
    "                loss_triplet.backward()\r\n",
    "\r\n",
    "                # Optimize\r\n",
    "                optimizer.step()\r\n",
    "                # optimizer.step(closure)\r\n",
    "\r\n",
    "\r\n",
    "                # Every 10 batches print out the loss\r\n",
    "                if i % 10 == 0 :\r\n",
    "                    print(f\"Epoch number {epoch}\\n Current loss {loss_triplet.item()}\\n\")\r\n",
    "                    iteration_number += 10\r\n",
    "                    counter.append(iteration_number)\r\n",
    "                    loss_history.append(loss_triplet.item())\r\n",
    "\r\n",
    "                if epoch %10 == 0:\r\n",
    "                    torch.save(net.state_dict(), MODEL_PATH)\r\n",
    "\r\n",
    "\r\n",
    "        show_plot(counter, loss_history)\r\n",
    "\r\n",
    "    elif MODE == 'test':\r\n",
    "        # Locate the test dataset and load it into the NetworkDataset\r\n",
    "        folder_dataset_test = datasets.ImageFolder(root=\"./data/testing/\")\r\n",
    "        test_dataset = NetworkDataset(imageFolderDataset=folder_dataset_test,\r\n",
    "                                                transform=transformation)\r\n",
    "        test_dataloader = DataLoader(test_dataset, num_workers=2, batch_size=1, shuffle=True)\r\n",
    "\r\n",
    "        # Grab one image that we are going to test\r\n",
    "        dataiter = iter(test_dataloader)\r\n",
    "        x0, _, _ = next(dataiter)\r\n",
    "\r\n",
    "        for i in range(5):\r\n",
    "            # Iterate over 5 images and test them with the first image (x0)\r\n",
    "            _, x1, x2 = next(dataiter)\r\n",
    "\r\n",
    "            # Concatenate the two images together\r\n",
    "            concatenated = torch.cat((x0, x1, x2), 0)\r\n",
    "            \r\n",
    "            output1, output2, output3 = net(x0.cuda(), x1.cuda(), x2.cuda())\r\n",
    "            euclidean_distance1 = F.pairwise_distance(output1, output2)\r\n",
    "            euclidean_distance2 = F.pairwise_distance(output1, output3)\r\n",
    "            imshow(torchvision.utils.make_grid(concatenated), f'Similarity: {euclidean_distance1.item():.2f}')\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3de59ade2ee4eabf8d6554090db31bdd94608df00f05391c2d316a7da62ee3f6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}